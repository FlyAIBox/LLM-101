{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附录 10.3: 搜索与检索 (Search & Retrieval)\n",
    "\n",
    "## 概述\n",
    "您知道可以使用 Claude 来**为您搜索维基百科**吗？Claude 可以查找和检索文章，然后您还可以使用 Claude 来总结和综合这些文章，从找到的内容中编写新颖的内容，等等。不仅仅是维基百科！您还可以搜索自己的文档，无论是存储为纯文本还是嵌入在向量数据库中。\n",
    "\n",
    "## 什么是 RAG (检索增强生成)?\n",
    "RAG (Retrieval-Augmented Generation) 是一种将外部知识检索与大语言模型生成能力相结合的技术：\n",
    "\n",
    "### RAG 的核心组件：\n",
    "1. **检索器 (Retriever)**: 从知识库中找到相关信息\n",
    "2. **生成器 (Generator)**: 基于检索到的信息生成回答\n",
    "3. **知识库 (Knowledge Base)**: 存储待检索的文档和数据\n",
    "\n",
    "### RAG 的优势：\n",
    "- **实时性**: 能够获取最新信息，不受模型训练时间限制\n",
    "- **准确性**: 基于真实数据源，减少幻觉现象\n",
    "- **可控性**: 可以指定特定的知识源\n",
    "- **可扩展性**: 随时更新知识库内容\n",
    "\n",
    "## 学习资源\n",
    "\n",
    "查看我们的 [RAG 实践教程示例](https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/Wikipedia/wikipedia-search-cookbook.ipynb) 来学习如何通过从向量数据库、维基百科、互联网等检索的数据来补充 Claude 的知识，提高 Claude 响应的准确性和相关性。在那里，您还可以学习如何使用某些 [嵌入技术 (embeddings)](https://docs.anthropic.com/claude/docs/embeddings) 和向量数据库工具。\n",
    "\n",
    "如果您有兴趣了解使用 Claude 的高级 RAG 架构，请查看我们的 [Claude 3 RAG 架构技术演示幻灯片](https://docs.google.com/presentation/d/1zxkSI7lLUBrZycA-_znwqu8DDyVhHLkQGScvzaZrUns/edit#slide=id.g2c736259dac_63_782)。\n",
    "\n",
    "## 技术架构图\n",
    "```\n",
    "用户查询 → 向量检索 → 相关文档 → Claude处理 → 生成回答\n",
    "    ↓           ↓          ↓         ↓         ↓\n",
    "  Query    Embeddings   Context   Generate  Response\n",
    "```\n",
    "\n",
    "## 后续章节预告\n",
    "在接下来的示例中，我们将演示：\n",
    "- 如何设置向量数据库\n",
    "- 实现文档嵌入和检索\n",
    "- 与 Claude API 集成\n",
    "- 构建完整的 RAG 应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境设置和依赖安装\n",
    "# 首先安装必要的依赖包\n",
    "\n",
    "# !pip install anthropic wikipedia-api sentence-transformers faiss-cpu numpy\n",
    "\n",
    "# 导入必要的库\n",
    "import anthropic\n",
    "import wikipedia\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "# 设置 Anthropic API 密钥\n",
    "# 请确保在环境变量中设置 ANTHROPIC_API_KEY\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\")  # 从环境变量获取API密钥\n",
    ")\n",
    "\n",
    "print(\"✅ 环境设置完成！\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. 基础维基百科搜索示例\n",
    "\n",
    "首先，我们来实现一个简单的维基百科搜索功能，让 Claude 能够获取并处理维基百科的内容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaSearcher:\n",
    "    \"\"\"\n",
    "    维基百科搜索器类\n",
    "    用于搜索、获取和处理维基百科文章内容\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, language='zh'):\n",
    "        \"\"\"\n",
    "        初始化维基百科搜索器\n",
    "        \n",
    "        Args:\n",
    "            language (str): 维基百科语言版本，默认为中文 'zh'\n",
    "        \"\"\"\n",
    "        wikipedia.set_lang(language)\n",
    "        self.language = language\n",
    "    \n",
    "    def search_articles(self, query: str, max_results: int = 5) -> List[str]:\n",
    "        \"\"\"\n",
    "        搜索维基百科文章标题\n",
    "        \n",
    "        Args:\n",
    "            query (str): 搜索关键词\n",
    "            max_results (int): 最大返回结果数量\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: 文章标题列表\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 搜索相关文章标题\n",
    "            titles = wikipedia.search(query, results=max_results)\n",
    "            print(f\"🔍 找到 {len(titles)} 篇相关文章:\")\n",
    "            for i, title in enumerate(titles, 1):\n",
    "                print(f\"  {i}. {title}\")\n",
    "            return titles\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 搜索失败: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_article_content(self, title: str, max_sentences: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        获取维基百科文章内容\n",
    "        \n",
    "        Args:\n",
    "            title (str): 文章标题\n",
    "            max_sentences (int): 最大返回句子数量\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: 包含标题、摘要、内容等信息的字典\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 获取文章页面\n",
    "            page = wikipedia.page(title)\n",
    "            \n",
    "            # 将内容分割为句子并限制数量\n",
    "            sentences = page.content.split('. ')[:max_sentences]\n",
    "            limited_content = '. '.join(sentences)\n",
    "            \n",
    "            article_info = {\n",
    "                'title': page.title,\n",
    "                'url': page.url,\n",
    "                'summary': page.summary,\n",
    "                'content': limited_content,\n",
    "                'length': len(page.content)\n",
    "            }\n",
    "            \n",
    "            print(f\"📄 成功获取文章: {page.title}\")\n",
    "            print(f\"📊 文章长度: {len(page.content)} 字符\")\n",
    "            \n",
    "            return article_info\n",
    "            \n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            print(f\"⚠️  存在歧义，可选择: {e.options[:5]}\")\n",
    "            # 选择第一个选项\n",
    "            return self.get_article_content(e.options[0], max_sentences)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 获取文章失败: {e}\")\n",
    "            return {}\n",
    "\n",
    "# 创建维基百科搜索器实例\n",
    "wiki_searcher = WikipediaSearcher()\n",
    "\n",
    "# 示例：搜索关于人工智能的文章\n",
    "search_query = \"人工智能\"\n",
    "articles = wiki_searcher.search_articles(search_query, max_results=3)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. 集成 Claude 进行内容分析\n",
    "\n",
    "现在我们将维基百科搜索结果与 Claude 结合，让 Claude 分析和总结检索到的内容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaudeRAGProcessor:\n",
    "    \"\"\"\n",
    "    Claude RAG 处理器\n",
    "    结合维基百科搜索和 Claude 分析能力\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, wiki_searcher: WikipediaSearcher):\n",
    "        \"\"\"\n",
    "        初始化 Claude RAG 处理器\n",
    "        \n",
    "        Args:\n",
    "            wiki_searcher (WikipediaSearcher): 维基百科搜索器实例\n",
    "        \"\"\"\n",
    "        self.wiki_searcher = wiki_searcher\n",
    "        \n",
    "    def search_and_analyze(self, user_query: str, max_articles: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        搜索相关文章并使用 Claude 进行分析\n",
    "        \n",
    "        Args:\n",
    "            user_query (str): 用户查询\n",
    "            max_articles (int): 最大搜索文章数量\n",
    "            \n",
    "        Returns:\n",
    "            str: Claude 分析后的响应\n",
    "        \"\"\"\n",
    "        print(f\"🚀 开始处理查询: {user_query}\")\n",
    "        \n",
    "        # 1. 搜索相关文章\n",
    "        article_titles = self.wiki_searcher.search_articles(user_query, max_articles)\n",
    "        \n",
    "        if not article_titles:\n",
    "            return \"❌ 未找到相关文章\"\n",
    "        \n",
    "        # 2. 获取文章内容\n",
    "        articles_content = []\n",
    "        for title in article_titles[:max_articles]:\n",
    "            article = self.wiki_searcher.get_article_content(title, max_sentences=8)\n",
    "            if article:\n",
    "                articles_content.append(article)\n",
    "        \n",
    "        # 3. 构建 Claude 提示词\n",
    "        context = self._build_context(articles_content)\n",
    "        prompt = self._build_prompt(user_query, context)\n",
    "        \n",
    "        # 4. 调用 Claude API\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-3-sonnet-20240229\",  # 使用 Claude 3 Sonnet 模型\n",
    "                max_tokens=1500,\n",
    "                temperature=0.3,  # 较低的温度以确保准确性\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            return response.content[0].text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"❌ Claude API 调用失败: {e}\"\n",
    "    \n",
    "    def _build_context(self, articles: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        构建上下文信息\n",
    "        \n",
    "        Args:\n",
    "            articles (List[Dict]): 文章列表\n",
    "            \n",
    "        Returns:\n",
    "            str: 格式化的上下文字符串\n",
    "        \"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, article in enumerate(articles, 1):\n",
    "            context_part = f\"\"\"\n",
    "【文章 {i}】\n",
    "标题: {article['title']}\n",
    "链接: {article['url']}\n",
    "摘要: {article['summary']}\n",
    "内容节选: {article['content'][:1000]}...\n",
    "\"\"\"\n",
    "            context_parts.append(context_part)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def _build_prompt(self, user_query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        构建 Claude 提示词\n",
    "        \n",
    "        Args:\n",
    "            user_query (str): 用户查询\n",
    "            context (str): 上下文信息\n",
    "            \n",
    "        Returns:\n",
    "            str: 完整的提示词\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "你是一个知识助手。基于以下从维基百科检索到的信息，请回答用户的问题。\n",
    "\n",
    "用户问题：{user_query}\n",
    "\n",
    "参考资料：\n",
    "{context}\n",
    "\n",
    "请根据上述资料回答用户问题，要求：\n",
    "1. 回答要准确、全面\n",
    "2. 如果资料中有具体数据或事实，请引用\n",
    "3. 保持客观和中立的语调\n",
    "4. 如果资料不足以完全回答问题，请说明\n",
    "5. 在回答末尾提供参考文章的链接\n",
    "\n",
    "回答：\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "# 创建 Claude RAG 处理器\n",
    "rag_processor = ClaudeRAGProcessor(wiki_searcher)\n",
    "\n",
    "# 示例查询\n",
    "user_question = \"什么是机器学习？它有哪些主要应用领域？\"\n",
    "print(f\"用户问题: {user_question}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行 RAG 查询示例\n",
    "# 注意：需要设置 ANTHROPIC_API_KEY 环境变量才能运行\n",
    "\n",
    "# 取消注释下面的代码来运行示例\n",
    "# result = rag_processor.search_and_analyze(user_question)\n",
    "# print(\"Claude 的回答：\")\n",
    "# print(result)\n",
    "\n",
    "# 模拟输出示例\n",
    "print(\"Claude 的回答：\")\n",
    "print(\"\"\"\n",
    "基于维基百科的资料，我来回答您关于机器学习的问题：\n",
    "\n",
    "**什么是机器学习？**\n",
    "\n",
    "机器学习是人工智能的一个分支，它是一种让计算机在没有明确编程的情况下学习的方法。机器学习算法通过分析数据来识别模式，并做出预测或决策。\n",
    "\n",
    "**主要应用领域：**\n",
    "\n",
    "1. **图像识别**: 人脸识别、医学影像分析、自动驾驶汽车的视觉系统\n",
    "2. **自然语言处理**: 机器翻译、语音识别、聊天机器人\n",
    "3. **推荐系统**: 电商平台、流媒体服务的个性化推荐\n",
    "4. **金融服务**: 风险评估、欺诈检测、算法交易\n",
    "5. **医疗健康**: 疾病诊断、药物发现、基因分析\n",
    "6. **交通运输**: 自动驾驶、交通优化、路线规划\n",
    "\n",
    "参考资料：\n",
    "- 机器学习 - https://zh.wikipedia.org/wiki/机器学习\n",
    "- 人工智能 - https://zh.wikipedia.org/wiki/人工智能\n",
    "- 深度学习 - https://zh.wikipedia.org/wiki/深度学习\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. 高级向量搜索与语义检索\n",
    "\n",
    "除了简单的关键词搜索，我们还可以使用向量嵌入技术实现更智能的语义搜索。这种方法能够理解查询的含义，而不仅仅是匹配关键词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearchEngine:\n",
    "    \"\"\"\n",
    "    向量搜索引擎\n",
    "    使用语义嵌入进行文档检索\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        \"\"\"\n",
    "        初始化向量搜索引擎\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): 句子嵌入模型名称\n",
    "        \"\"\"\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.documents = []  # 存储文档\n",
    "        self.embeddings = None  # 存储文档嵌入\n",
    "        self.index = None  # FAISS 索引\n",
    "        \n",
    "        print(f\"✅ 向量搜索引擎初始化完成，使用模型: {model_name}\")\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        添加文档到搜索引擎\n",
    "        \n",
    "        Args:\n",
    "            documents (List[Dict]): 文档列表，每个文档包含 'title', 'content' 等字段\n",
    "        \"\"\"\n",
    "        print(f\"📚 正在添加 {len(documents)} 个文档...\")\n",
    "        \n",
    "        # 存储文档\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        # 准备用于嵌入的文本\n",
    "        texts_for_embedding = []\n",
    "        for doc in documents:\n",
    "            # 组合标题和内容用于嵌入\n",
    "            text = f\"{doc.get('title', '')} {doc.get('content', '')}\".strip()\n",
    "            texts_for_embedding.append(text)\n",
    "        \n",
    "        # 生成嵌入向量\n",
    "        print(\"🔄 正在生成文档嵌入向量...\")\n",
    "        new_embeddings = self.encoder.encode(texts_for_embedding)\n",
    "        \n",
    "        # 更新嵌入矩阵\n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = new_embeddings\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embeddings])\n",
    "        \n",
    "        # 重建 FAISS 索引\n",
    "        self._build_index()\n",
    "        \n",
    "        print(f\"✅ 文档添加完成，当前共有 {len(self.documents)} 个文档\")\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"构建 FAISS 索引用于快速相似度搜索\"\"\"\n",
    "        if self.embeddings is not None:\n",
    "            dimension = self.embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dimension)  # 使用内积相似度\n",
    "            \n",
    "            # 归一化嵌入向量\n",
    "            faiss.normalize_L2(self.embeddings)\n",
    "            self.index.add(self.embeddings)\n",
    "            \n",
    "            print(f\"🔍 FAISS 索引构建完成，维度: {dimension}\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        语义搜索文档\n",
    "        \n",
    "        Args:\n",
    "            query (str): 查询文本\n",
    "            top_k (int): 返回最相似的文档数量\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: 搜索结果，包含文档和相似度分数\n",
    "        \"\"\"\n",
    "        if self.index is None or len(self.documents) == 0:\n",
    "            print(\"❌ 搜索引擎为空，请先添加文档\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"🔍 搜索查询: '{query}'\")\n",
    "        \n",
    "        # 对查询进行嵌入\n",
    "        query_embedding = self.encoder.encode([query])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # 执行搜索\n",
    "        scores, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # 整理搜索结果\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx < len(self.documents):  # 确保索引有效\n",
    "                result = {\n",
    "                    'rank': i + 1,\n",
    "                    'score': float(score),\n",
    "                    'document': self.documents[idx]\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        print(f\"📊 找到 {len(results)} 个相关文档\")\n",
    "        return results\n",
    "    \n",
    "    def print_search_results(self, results: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        打印搜索结果\n",
    "        \n",
    "        Args:\n",
    "            results (List[Dict]): 搜索结果\n",
    "        \"\"\"\n",
    "        for result in results:\n",
    "            print(f\"\\n排名 {result['rank']} (相似度: {result['score']:.3f})\")\n",
    "            print(f\"标题: {result['document'].get('title', 'N/A')}\")\n",
    "            print(f\"内容摘要: {result['document'].get('content', 'N/A')[:200]}...\")\n",
    "            if 'url' in result['document']:\n",
    "                print(f\"链接: {result['document']['url']}\")\n",
    "\n",
    "# 创建向量搜索引擎实例\n",
    "vector_engine = VectorSearchEngine()\n",
    "\n",
    "# 示例：准备一些文档数据\n",
    "sample_documents = [\n",
    "    {\n",
    "        'title': '深度学习基础',\n",
    "        'content': '深度学习是机器学习的一个分支，它使用多层神经网络来学习数据的表示。深度学习在图像识别、自然语言处理和语音识别等领域取得了突破性进展。',\n",
    "        'category': 'AI技术',\n",
    "        'url': 'https://example.com/deep-learning'\n",
    "    },\n",
    "    {\n",
    "        'title': '自然语言处理技术',\n",
    "        'content': '自然语言处理(NLP)是人工智能领域的一个重要分支，旨在让计算机理解和生成人类语言。包括文本分析、机器翻译、情感分析等应用。',\n",
    "        'category': 'AI应用',\n",
    "        'url': 'https://example.com/nlp'\n",
    "    },\n",
    "    {\n",
    "        'title': '计算机视觉原理',\n",
    "        'content': '计算机视觉是让计算机获得类似人类视觉系统的能力，包括图像识别、目标检测、图像分割等技术。广泛应用于自动驾驶、医疗诊断等领域。',\n",
    "        'category': 'AI应用',\n",
    "        'url': 'https://example.com/cv'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"准备添加示例文档到向量搜索引擎...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示向量搜索功能\n",
    "# 注意：实际运行需要安装 sentence-transformers 和 faiss-cpu\n",
    "\n",
    "# 取消注释下面的代码来运行向量搜索示例\n",
    "# vector_engine.add_documents(sample_documents)\n",
    "\n",
    "# 执行不同类型的语义搜索\n",
    "# search_queries = [\n",
    "#     \"如何让机器理解人类语言？\",\n",
    "#     \"图像识别相关技术\",\n",
    "#     \"神经网络和深度学习\"\n",
    "# ]\n",
    "\n",
    "# for query in search_queries:\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     results = vector_engine.search(query, top_k=2)\n",
    "#     vector_engine.print_search_results(results)\n",
    "\n",
    "# 模拟向量搜索输出\n",
    "print(\"向量搜索引擎演示：\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n🔍 搜索查询: '如何让机器理解人类语言？'\")\n",
    "print(\"📊 找到 2 个相关文档\")\n",
    "print(\"\\n排名 1 (相似度: 0.758)\")\n",
    "print(\"标题: 自然语言处理技术\")\n",
    "print(\"内容摘要: 自然语言处理(NLP)是人工智能领域的一个重要分支，旨在让计算机理解和生成人类语言。包括文本分析、机器翻译、情感分析等应用。\")\n",
    "print(\"链接: https://example.com/nlp\")\n",
    "\n",
    "print(\"\\n排名 2 (相似度: 0.632)\")\n",
    "print(\"标题: 深度学习基础\")\n",
    "print(\"内容摘要: 深度学习是机器学习的一个分支，它使用多层神经网络来学习数据的表示。深度学习在图像识别、自然语言处理和语音识别等领域取得了突破性进展。\")\n",
    "print(\"链接: https://example.com/deep-learning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\n🔍 搜索查询: '图像识别相关技术'\")\n",
    "print(\"📊 找到 2 个相关文档\")\n",
    "print(\"\\n排名 1 (相似度: 0.812)\")\n",
    "print(\"标题: 计算机视觉原理\")\n",
    "print(\"内容摘要: 计算机视觉是让计算机获得类似人类视觉系统的能力，包括图像识别、目标检测、图像分割等技术。广泛应用于自动驾驶、医疗诊断等领域。\")\n",
    "print(\"链接: https://example.com/cv\")\n",
    "\n",
    "print(\"\\n排名 2 (相似度: 0.689)\")\n",
    "print(\"标题: 深度学习基础\")  \n",
    "print(\"内容摘要: 深度学习是机器学习的一个分支，它使用多层神经网络来学习数据的表示。深度学习在图像识别、自然语言处理和语音识别等领域取得了突破性进展。\")\n",
    "print(\"链接: https://example.com/deep-learning\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. RAG 系统设计最佳实践\n",
    "\n",
    "### 🏗️ 系统架构设计原则\n",
    "\n",
    "#### 4.1 数据预处理\n",
    "- **文档分块**: 将长文档分割成适当大小的块（通常 200-500 tokens）\n",
    "- **内容清洗**: 去除无关格式、标签和噪音\n",
    "- **元数据提取**: 保留标题、日期、来源等重要信息\n",
    "\n",
    "#### 4.2 嵌入策略\n",
    "- **模型选择**: 根据语言和领域选择合适的嵌入模型\n",
    "- **批量处理**: 批量生成嵌入以提高效率\n",
    "- **增量更新**: 支持新文档的增量添加\n",
    "\n",
    "#### 4.3 检索优化\n",
    "- **混合搜索**: 结合关键词搜索和语义搜索\n",
    "- **重排序**: 使用交叉编码器对检索结果重新排序\n",
    "- **多样性**: 确保检索结果的多样性，避免信息重复\n",
    "\n",
    "#### 4.4 生成策略\n",
    "- **上下文管理**: 控制输入给 LLM 的上下文长度\n",
    "- **引用机制**: 在回答中包含信息来源\n",
    "- **一致性检查**: 验证生成内容与检索内容的一致性\n",
    "\n",
    "### 📊 性能评估指标\n",
    "\n",
    "1. **检索质量**\n",
    "   - Recall@K: 前K个结果中相关文档的比例\n",
    "   - Precision@K: 前K个结果中正确答案的比例\n",
    "   - MRR (Mean Reciprocal Rank): 平均倒数排名\n",
    "\n",
    "2. **生成质量**\n",
    "   - BLEU/ROUGE: 与参考答案的重叠度\n",
    "   - BERTScore: 语义相似度评分\n",
    "   - 人工评估: 准确性、流畅性、相关性\n",
    "\n",
    "3. **系统性能**\n",
    "   - 响应时间: 端到端查询处理时间\n",
    "   - 吞吐量: 每秒处理的查询数量\n",
    "   - 资源使用: CPU、内存、存储占用\n",
    "\n",
    "### 🔧 部署建议\n",
    "\n",
    "#### 技术栈推荐\n",
    "```python\n",
    "# 向量数据库选择\n",
    "# - FAISS: 适合原型开发和小规模部署\n",
    "# - Pinecone: 托管向量数据库服务\n",
    "# - Weaviate: 开源向量数据库\n",
    "# - Qdrant: 高性能向量搜索引擎\n",
    "\n",
    "# 嵌入模型选择\n",
    "# - 中文: text2vec-chinese, uer/sbert-base-chinese-nli\n",
    "# - 英文: all-MiniLM-L6-v2, sentence-transformers/all-mpnet-base-v2\n",
    "# - 多语言: multilingual-e5-large\n",
    "\n",
    "# Web框架\n",
    "# - FastAPI: 高性能异步框架\n",
    "# - Flask: 轻量级简单框架\n",
    "# - Django: 功能完整的Web框架\n",
    "```\n",
    "\n",
    "#### 生产环境考虑\n",
    "- **缓存策略**: 缓存常见查询结果\n",
    "- **负载均衡**: 多实例部署以处理高并发\n",
    "- **监控告警**: 实时监控系统性能和错误率\n",
    "- **安全防护**: API 限流、身份验证、数据加密\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. 总结与进阶学习\n",
    "\n",
    "### 🎯 本章关键要点\n",
    "\n",
    "1. **RAG 核心概念**: 检索增强生成技术结合了信息检索和文本生成的优势\n",
    "2. **实现方法**: 从简单的维基百科搜索到复杂的向量语义检索\n",
    "3. **技术栈**: Claude API + 向量数据库 + 嵌入模型的完整解决方案\n",
    "4. **最佳实践**: 数据预处理、检索优化、生成策略的系统设计\n",
    "\n",
    "### 🚀 进阶学习路径\n",
    "\n",
    "#### 初学者 (1-2周)\n",
    "- [ ] 掌握 RAG 基本概念和工作流程\n",
    "- [ ] 实现简单的文档问答系统\n",
    "- [ ] 学习使用 FAISS 进行向量搜索\n",
    "- [ ] 了解不同的嵌入模型特点\n",
    "\n",
    "#### 中级开发者 (2-4周)\n",
    "- [ ] 优化检索性能和生成质量\n",
    "- [ ] 实现混合搜索（关键词+语义）\n",
    "- [ ] 学习使用专业向量数据库\n",
    "- [ ] 设计评估指标和测试框架\n",
    "\n",
    "#### 高级工程师 (1-2月)\n",
    "- [ ] 构建生产级 RAG 系统\n",
    "- [ ] 实现多模态检索（文本+图像）\n",
    "- [ ] 优化大规模部署和性能\n",
    "- [ ] 研究前沿 RAG 架构和技术\n",
    "\n",
    "### 📚 推荐资源\n",
    "\n",
    "#### 官方文档\n",
    "- [Anthropic Claude API 文档](https://docs.anthropic.com/claude/docs/intro-to-claude)\n",
    "- [LangChain RAG 教程](https://python.langchain.com/docs/use_cases/question_answering)\n",
    "- [Sentence Transformers 文档](https://www.sbert.net/)\n",
    "\n",
    "#### 开源项目\n",
    "- [RAG Cookbook](https://github.com/anthropics/anthropic-cookbook) - Anthropic 官方示例\n",
    "- [LlamaIndex](https://github.com/run-llama/llama_index) - RAG 框架\n",
    "- [Haystack](https://github.com/deepset-ai/haystack) - NLP 管道工具\n",
    "\n",
    "#### 学术论文\n",
    "- \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al., 2020)\n",
    "- \"Dense Passage Retrieval for Open-Domain Question Answering\" (Karpukhin et al., 2020)\n",
    "- \"FiD: Leveraging Passage Retrieval with Generative Models\" (Izacard & Grave, 2021)\n",
    "\n",
    "### 💡 实践建议\n",
    "\n",
    "1. **从小做起**: 先在小数据集上验证概念，再扩展到生产环境\n",
    "2. **迭代优化**: 持续收集用户反馈，优化检索和生成效果\n",
    "3. **监控指标**: 建立完善的监控体系，及时发现和解决问题\n",
    "4. **安全第一**: 注意数据隐私和模型安全，防止恶意攻击\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 恭喜完成搜索与检索技术的学习！**\n",
    "\n",
    "现在您已经掌握了使用 Claude 构建智能搜索系统的核心技能。继续探索和实践，将这些技术应用到您的项目中吧！\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
