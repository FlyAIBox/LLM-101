# 《零基础实战大模型》课程大纲

------

## 模块一：大模型实战入门与环境搭建

说明：

本模块旨在为学员搭建扎实的大模型开发基础，从理解大模型的基本概念、发展趋势与应用场景入手，逐步掌握Python开发环境的配置、GPU加速环境的搭建以及版本控制工具Git的使用。通过初步调用主流大模型的API，学员将建立起与大模型交互的基本能力，为后续深入学习打下坚实基础。

**学习目标：**

- 理解大模型（LLM）的核心概念、发展历程与主流模型。
- 掌握Python开发环境、GPU驱动与CUDA的正确配置方法。
- 熟悉Git与GitHub基础操作，进行代码版本管理。
- 能够独立调用主流大模型API完成基础文本生成任务。

**详细内容：**

1. **大模型基础概念**
   - 什么是大模型 (LLM)？主流LLM架构（Transformer）简介。
   - 当前主流大模型介绍：**OpenAI GPT系列、Ali Qwen系列、深度求索 DeepSeek模型**等。
   - 大模型的应用场景与商业价值。
2. **大模型开发环境搭建**
   - Python环境配置与包管理：**Conda (Anaconda/Miniconda)**。
   - GPU驱动与CUDA安装 (NVIDIA GPU)：**CUDA Toolkit、cuDNN**。
   - 常用IDE：**Cursor** (推荐)、远程SSH和调试。
3. **大模型API调用初探**
   - API Key管理与安全实践。
   - 使用Curl请求DeepSeek API
   - 请求与响应格式解析：**JSON**。

**实践练习：**

1. **实践一：配置本地大模型开发环境**
   - 任务：在个人电脑上成功配置Anaconda/Miniconda、Python环境，并安装必要的库。
   - 任务：检查并确保GPU驱动和CUDA Toolkit安装正确，能被PyTorch等框架识别。
   - 任务：初始化一个Git仓库，并将其推送到GitHub。
2. **实践二：调用主流大模型API**
   - 任务：申请一个OpenAI API Key（或其他国内主流大模型API Key）。
   - 任务：编写Python代码，成功调用API完成以下至少两种文本生成任务：
     - 一段关于“未来科技”的短文。
     - 将一段英文翻译成中文。
     - 对一篇新闻文章进行摘要。

------

## 模块二：大模型基础推理与提示词工程

说明：

本模块深入探讨大模型的核心推理机制和提示词工程的艺术。学员将理解大模型如何生成文本，掌握各种解码策略，并学习如何设计高效、精准的提示词，以最大化模型的性能。重点将放在思维链（CoT）、**自我反思（Self-Reflection）等高级技巧，以及函数调用（Function Calling）**的实战应用，为构建复杂AI应用奠定基础。

**学习目标：**

- 理解大模型文本生成的基本原理和解码策略。
- 掌握基础提示词设计原则，能够有效引导模型完成任务。
- 熟练运用CoT、自我反思等高级提示词技巧。
- 理解并实践大模型函数调用（Function Calling）机制，实现与外部工具的交互。

**详细内容：**

1. **大模型推理基础**
   - 文本生成原理概述：自回归生成、条件生成。
   - **Tokenization**与**Embedding**。
   - 解码策略：**贪婪解码、束搜索、Top-k、Top-p、温度参数 (temperature)**。
2. **提示词工程 (Prompt Engineering) 核心概念**
   - 什么是提示词工程？为什么重要？
   - 基础提示词设计原则：**清晰、具体、角色设定、格式化输出要求**。
   - **Few-shot prompting (少样本学习)** 与 **Zero-shot prompting (零样本学习)**。
3. **高阶提示词技巧**
   - **思维链 (Chain of Thought, CoT)**：通过分解步骤引导模型逐步推理。
   - **自我反思 (Self-Reflection)**：让模型自我评估和改进输出。
   - 提示模板设计：**Jinja2模板引擎、LangChain Prompt Templates**。
   - 动态Prompt生成：根据用户输入或上下文动态构造Prompt。
   - 提示词如何写、管理和优化（FIXME）
4. **大模型调用方式与函数调用 (Function Calling)**
   - **OpenAI Function Calling 原理与实战**：
     - 使用**JSON Schema**定义工具参数结构。
     - **Tool Call + Response Chain** 流程。
     - 示例场景：天气查询、数据库查询、API 调用。

**实践练习：**

1. **实践一：高级提示词设计**
   - 任务：针对同一任务（例如：为一篇电商产品描述润色），分别使用以下提示词技巧进行生成，并对比结果：
     - 直接提问（零样本）。
     - 提供2-3个示例（少样本）。
     - 引导模型逐步思考（思维链）。
     - 让模型生成后进行自我评估和改进（自我反思）。
2. **实践二：Function Calling 实战**
   - 任务：构建一个简单的天气查询机器人，利用Function Calling调用一个虚拟的或真实的**第三方天气API**（如和风天气API，仅需模拟请求响应即可）。
   - 要求：用户输入“明天上海天气如何？”时，模型能识别意图并调用天气工具，返回模拟的天气数据。

------

## 模块三：VLLM 高性能模型推理与压测

说明：

本模块聚焦大模型在生产环境中的高性能推理与部署。学员将理解大模型推理的性能瓶颈，学习如何利用VLLM等先进推理框架提升吞吐量和降低延迟。通过实际部署开源大模型并进行压测，学员将掌握性能评估、优化以及不同私有化部署方案的对比，为企业级应用落地提供技术支撑。

**学习目标：**

- 理解大模型推理的性能挑战及优化策略。
- 掌握VLLM的原理、安装与高性能推理实践。
- 学会使用压测工具评估大模型服务性能指标。
- 能够对比并选择合适的本地/私有化部署方案。

**详细内容：**

1. **大模型推理性能优化挑战**
   - 显存占用、推理延迟、吞吐量瓶颈分析。
   - **批处理 (Batching)** 与 **KV Cache** (Paged Attention原理)。
2. **VLLM：高性能大模型推理库**
   - VLLM核心原理与优势：**Paged Attention**、连续批处理。
   - VLLM安装与配置：**pip安装**、Docker部署。
   - 支持的模型与硬件要求：**NVIDIA GPU (非硬性)**。
3. **VLLM模型推理与压测**
   - VLLM RESTful API部署与调用。
   - VLLM模型压测：TTFT、吞吐量、并发数（FIXME）
   - 性能监控与分析：**NVIDIA-SMI** (GPU利用率、显存占用)。
4. **本地/私有化部署调用方式对比**
   - **HuggingFace Transformers (pipeline)**：易用性、灵E活性。
   - **TGI (Text Generation Inference)**：HuggingFace官方推荐的推理服务。
   - **Ollama**：本地LLM一站式解决方案，轻量易用。

**实践练习：**

1. **实践一：VLLM本地部署与推理**
   - 任务：在本地成功部署并运行一个基于VLLM的**Llama 3 8B Instruct**或**Qwen-1.5-7B-Chat**模型推理服务。
   - 任务：通过Python客户端调用VLLM的API，生成一段指定长度的中文文本，并记录推理时间。
2. **实践二：VLLM服务压测**
   - 任务：使用对部署的VLLM服务进行压测。
   - 要求：设计不同并发用户数（例如：1, 5, 10, 20）下的测试场景，记录并分析**QPS (Queries Per Second)** 和平均**延迟**，并尝试找出性能瓶颈。

------

## 模块四：模型上下文协议（MCP）实战

说明：

**学习目标：**

**详细内容：**

1. **模型上下文协议 (Model Context Protocol, MCP) 实践**

   - 背景与重要性：大模型上下文限制与高效利用。

   - MCP设计思想：**结构化、可扩展的上下文组织**，如Markdown表格、特定XML/JSON格式。

   - 如何将检索到的信息有效组织并传递给大模型：**Prompt Template中嵌入检索结果**。



**实践练习：**

---

## 模块五：RAG（检索增强生成）实战

说明：

本模块深入讲解检索增强生成（RAG）技术，它是解决大模型知识截止和幻觉问题的关键。学员将学习RAG的核心原理、架构，掌握向量数据库和Embedding模型的使用。通过LangChain框架，实现从文档加载到向量存储、检索、重排序和生成全流程。同时，引入模型上下文协议（MCP），优化大模型对检索结果的利用，最终实战构建一个企业内部技术文档智能问答系统。

**学习目标：**

- 理解RAG的原理与价值，解决大模型知识边界问题。
- 掌握向量数据库（Milvus和Qdrant）和Embedding模型的使用。
- 熟练运用LangChain构建RAG管道。
- 理解并实践模型上下文协议（MCP），优化大模型上下文利用效率。
- 能够构建基于企业私有知识库的智能问答系统。

**详细内容：**

1. **RAG (Retrieval Augmented Generation) 核心原理**
   - 传统大模型局限性：知识截止、幻觉问题。
   - RAG如何解决这些问题：外部知识引入。
   - RAG架构与工作流程：**文档加载 → 分块 → Embedding → 存储 → 检索 → 重排序 → 生成**。
2. **向量数据库与Embedding模型**
   - 向量数据库介绍：**ChromaDB** (本地易用)、**PGVector** (PostgreSQL扩展)。
   - Embedding模型选择与使用：**OpenAI text-embedding-3-large/small, BGE-large-zh-v1.5, E5**。
3. **LangChain与LlamaIndex全流程实践**
   - **LangChain 核心组件详解**：
     - **Chains (LLMChain, RetrievalQAChain)**：基本流程封装。
     - **Retrievers**：向量相似度检索、BM25稀疏检索。
     - **Memory (ConversationBufferWindowMemory)**：短期对话记忆。
   - **LlamaIndex 全流程实践**：
     - **Document Loaders**：PDF、网页、SQL、Notion等格式支持。
     - **Indexing (VectorStoreIndex)**：向量化存储、元数据管理。
     - **Query Engine (RetrieverQueryEngine)**：检索 + 生成一体化。
4. **模型上下文协议 (Model Context Protocol, MCP) 实践**
   - 背景与重要性：大模型上下文限制与高效利用。
   - MCP设计思想：**结构化、可扩展的上下文组织**，如Markdown表格、特定XML/JSON格式。
   - 如何将检索到的信息有效组织并传递给大模型：**Prompt Template中嵌入检索结果**。

**实践练习：**

1. **实践一：构建企业内部技术文档RAG**
   - 任务：收集并处理一份企业内部技术文档（如Markdown格式的API文档或开发规范，可自拟或使用开源文档）。
   - 任务：使用**LangChain/LlamaIndex**，选择一个合适的Embedding模型（如**BGE-large-zh-v1.5**），将文档分块并向量化，存储到**ChromaDB**中。
   - 任务：实现基于该知识库的RAG问答功能，用户输入问题后，系统能检索相关文档片段并生成回答。
2. **实践二：MCP优化RAG上下文**
   - 任务：在实践一的基础上，设计一个简单的MCP方案，例如，将检索到的文档片段以Markdown列表或表格的形式嵌入到Prompt中。
   - 任务：对比使用MCP前后，大模型在回答复杂技术问题（需要结合多个文档片段）时的准确性和连贯性。

------

## 模块六：大模型Agent 实战

说明：

本模块将聚焦大模型Agent的构建与协作，学员将学习Agent的核心概念、组件及主流框架。通过LangChain Agents、AutoGen和Ango的深度实践，掌握如何设计工具、规划任务以及实现多Agent之间的协同。最终，通过构建一个智慧园区通行管理助理，学员将掌握如何在企业实际场景中，利用Agent实现复杂任务的自动化和流程优化。

**学习目标：**

- 理解大模型Agent的原理、组成要素及应用场景。
- 掌握LangChain Agents中ReAct、Plan-and-Execute等Agent类型及工具集成。
- 掌握 Agno 框架的核心概念与使用方法，理解其在自主智能体构建中的优势。
- 熟练运用AutoGen框架实现多Agent协作，包括角色定义、群聊模式与辩论机制。
- 具备设计Agent状态机和多轮对话逻辑的能力。
- 能够构建企业级多Agent协作系统，解决实际业务问题。

**详细内容：**

1. **大模型Agent概念与架构**
   - 什么是Agent？为什么需要Agent？**自主性、响应性、社会性、能动性**。
   - Agent组成要素：**LLM、Memory、Tools、Planning**。
   - 主流Agent框架介绍：**LangChain Agents、AutoGPT (概念)、AutoGen**。
2. **LangChain Agents 深度实践**
   - **Agents 类型**：
     - **ReAct Agent (ZeroShotReactDescriptionAgent)**：推理+行动循环。
     - **Plan-and-Execute Agent**：先规划再执行。
   - **Tools**：集成自定义工具 (如HTTP请求工具、数据库查询工具、Python代码执行工具)。
3. **AutoGen 多 Agent 协作框架**
   - Agent 角色定义：**UserProxyAgent、AssistantAgent、GroupChatManager**。
   - **Group Chat模式**：多Agent轮流发言、达成共识。
   - **Debate机制**：Agent间进行辩论以提升决策质量。
   - 通信机制：**Message Passing、工具调用、反馈循环**。
4. **Agno 框架入门与实战**

   - Agno 的设计哲学：面向自主智能体的开发，强调规划与执行。
   - Agno 的核心组件：智能体、工具、任务、环境。
   - Agno 的优势：高性能、模型无关性、原生多模态、内置 RAG/记忆。

   - Agno 环境搭建与基本使用：
   - **Agno 智能体间的协作模式：**
     - 单智能体任务执行。
     - 多智能体协同工作原理（`agno.team.Team`，任务分解、结果汇总）。
5. **Agent 构建与多轮对话逻辑设计**

   - 如何设计一个 Agent 的状态机？
   - 多轮对话中如何保持上下文？
   - 如何设计 Agent 之间的消息传递协议？

**实践练习：**

1. **实践一：基于LangChain的单Agent工具调用**
   - 任务：构建一个简单的单Agent，使其能够通过调用一个工具来完成任务。例如：
     - **网页内容摘要Agent**：利用LangChain的`requests_tool`或`PlaywrightBrowser`工具，接收URL，访问网页并生成摘要。
     - **天气查询Agent**：集成在模块二中实现的函数调用工具，处理更复杂的意图和多轮对话。
2. **实践三：基于 Agno 的 AI 旅行助手**
   - **任务：** 使用 Agno 框架，设计并实现一个 AI 旅行助手的最小原型，展示其研究和规划能力。
   - **系统组件：**
     - **研究员智能体 (Researcher Agent)：**
       - **职责：** 负责根据用户偏好搜索旅行目的地、活动和住宿信息。
       - **工具：** 使用 **SerpAPI**（或模拟其功能）进行网络搜索，获取最新旅行资讯、景点评价、酒店价格等。
       - **输出：** 分析搜索结果并返回最相关的信息摘要。
     - **规划师智能体 (Planner Agent)：**
       - **职责：** 基于研究结果生成详细的行程计划。
       - **功能：** 确保行程结构良好、信息丰富且具有吸引力，提供平衡的建议，包含事实引用。
       - **依赖：** 接收研究员智能体提供的原始信息，并进行整合、筛选和优化，形成可行的旅行方案。
   - **核心功能：**
     - 用户输入旅行偏好（如：目的地、预算、天数、偏好活动类型）。
     - 研究员智能体进行信息收集。
     - 规划师智能体基于收集到的信息生成详细的旅行计划。
3. **实践二：智慧园区通行管理助理原型**
   - 任务：使用**AutoGen**框架，设计并实现一个智慧园区通行管理助理的最小原型。
   - **核心功能**：模拟访客预约和内部人员权限查询。
   - **Agent角色**：
     - **UserProxyAgent**：模拟用户提问。
     - **访客预约Agent (AssistantAgent)**：负责接收访客信息（姓名、来访时间、被访人），并模拟调用一个虚拟的“预约API”。
     - **权限查询Agent (AssistantAgent)**：负责接收人员姓名，并模拟调用一个虚拟的“权限查询API”返回其通行权限。
   - **协作流程**：当用户提出复杂请求（如：“帮我预约一位名叫张三的访客，他明天下午两点来，他要找李四。然后帮我查一下我的停车区域权限。”），UserProxyAgent将任务分配给不同的Agent协作完成。

------

## 模块七：n8n（AI Workflow Automation）实战

说明：

本模块将介绍n8n这一强大的AI工作流自动化平台，旨在帮助学员脱离代码，通过可视化界面快速编排和部署复杂的AI工作流。学员将学习n8n的环境搭建、基础操作，以及如何集成大模型API。通过构建企业级自动化工作流，例如社交媒体内容审核或客户反馈智能分类，学员将掌握利用n8n实现业务流程自动化的能力。

**学习目标：**

- 理解n8n作为AI工作流自动化平台的价值与核心概念。
- 掌握n8n环境搭建（Docker）及UI界面的基础操作。
- 能够使用n8n的节点调用大模型API，并编排数据流。
- 能够设计并实现基于大模型的自动化业务工作流。

**详细内容：**

1. **n8n 简介与核心概念**
   - 什么是n8n？**可视化工作流自动化平台**。
   - 核心概念：**节点 (Nodes)**、**工作流 (Workflows)**、**触发器 (Triggers)**、**操作 (Operations)**。
   - n8n与大模型结合的优势：**快速编排、降低代码量、可视化管理**。
2. **n8n环境搭建与基础操作**
   - **Docker部署n8n** (推荐)。
   - n8n UI界面介绍与导航。
   - 基础工作流构建：**Webhook触发、HTTP请求节点、数据转换、条件判断**。
3. **n8n与大模型集成实战**
   - 使用n8n的**HTTP Request节点**或**OpenAI节点**调用大模型API。
   - 数据流与变量管理。
4. **n8n高级应用与实战**
   - 自定义节点开发 (JavaScript/Python Code节点)。
   - 错误处理与日志记录。
   - 结合RAG和Agent：在n8n中编排复杂的AI工作流。

**实践练习：**

1. **实践一：自动化社交媒体内容审核工作流**
   - 任务：使用n8n构建一个工作流。
   - 触发器：模拟接收到新的社交媒体内容（例如通过**Webhook**节点接收一个JSON请求）。
   - 操作：调用大模型API（例如**OpenAI GPT-4o**）对内容进行审核，判断是否包含敏感词汇或不当内容。
   - 分支：根据大模型的审核结果，如果内容违规，发送一封邮件通知审核人员；如果合规，则模拟发布（例如打印到控制台或发送到Mock API）。
2. **实践二：客户反馈智能分类与派发工作流**
   - 任务：设计并实现一个n8n工作流，自动化处理客户反馈。
   - 触发器：模拟接收客户邮件或表单提交。
   - 操作：大模型抽取关键信息（如：客户姓名、问题类型、紧急程度、产品名称）。
   - 操作：根据问题类型和紧急程度，自动派发到不同部门的负责人邮箱或内部工单系统（通过n8n的**Email Send Node**或**HTTP Request Node**调用虚拟工单API）。

------

## 模块八：大模型高效微调工具实战

说明：

本模块将深入探讨大模型微调技术，特别是高效参数微调（PEFT）方法。学员将学习微调的必要性、LoRA、Q-LoRA等主流PEFT原理，并掌握Usloth、LlamaFactory这一强大的微调工具。通过实际操作，学员将对开源大模型进行LoRA微调，并学会评估和部署微调后的模型，从而实现模型的领域特化和性能优化。

**学习目标：**

- 理解大模型微调的重要性，区分全量微调与PEFT。
- 掌握LoRA、Q-LoRA等主流PEFT方法的原理与优势。
- 熟练使用LlamaFactory进行开源大模型的PEFT微调。
- 学会评估微调模型的效果，并将其部署到推理服务。

**详细内容：**

1. **大模型微调概述**
   - 为什么需要微调？(领域适应、任务特化、性能优化、成本控制)。
   - **全量微调 (Full Fine-tuning)** 与 **高效参数微调 (PEFT)** 对比。
2. **PEFT (Parameter-Efficient Fine-Tuning) 方法**
   - **LoRA (Low-Rank Adaptation) 原理与优势**：低秩矩阵分解、减少训练参数量。
   - **Q-LoRA (Quantized LoRA)**：量化后LoRA，进一步降低显存占用。
   - **P-Tuning v2**：基于可学习Prompt的轻量微调。
   - **Adapter**：插入小型神经网络模块进行增量训练。
   - 适用场景分析：何时选择哪种微调方式？
3. **LlamaFactory (LLaMA-Factory) 入门**
   - LlamaFactory简介与功能：**一站式大模型微调训练框架**。
   - 环境搭建与依赖安装：**Python依赖、PyTorch、Transformers**。
   - 数据集准备与格式要求：**Alpaca格式 (JSONL)**。
4. **LlamaFactory PEFT微调实战**
   - 配置文件参数详解：**学习率、batch size、epoch、LoRA秩、Alpha值**等。
   - 监控微调过程：**TensorBoard** (Loss曲线、评估指标)。
   - 微调模型保存与加载：**Adapter模型保存与合并 (merge_and_unload)**。
5. **微调模型评估与部署**
   - 评估指标：**Perplexity、Rouge、BLEU** (生成任务)，**准确率、F1 Score** (分类任务)。

**实践练习：**

1. **实践一：法律领域开源大模型LoRA微调**
   - 任务：选择一个开源大模型（如**Qwen-1.5-7B-Chat / Llama 3 8B-Instruct**）。
   - 任务：收集一个小型（例如：100-200条问答对）的法律领域数据集（可自制或从公开数据集中抽取，例如：关于合同法、劳动法的常见问题解答）。
   - 任务：使用LlamaFactory对该模型进行**LoRA微调**。
   - 任务：监控训练过程中的Loss变化。
2. **实践二：微调模型效果评估与部署**
   - 任务：使用测试集评估微调前后模型在该法律领域问答任务上的表现，进行**人工评估**（判断回答的准确性、专业性）和**量化指标**（若有合适的评估工具）。
   - 任务：将微调后的Adapter模型与原模型合并，并使用**VLLM**或其他推理框架进行部署，通过API测试其在法律领域问答上的能力。

------

## 模块九：企业级大模型项目落地：融合与部署

说明：

本模块是整个课程的终极挑战，旨在指导学员将前面学到的所有技术融会贯通，从需求分析到架构设计、代码实现、项目部署和后续迭代规划，完整地走过一个企业级大模型项目的生命周期。通过一个企业级智能法律咨询助手的综合实战，学员将全面掌握大模型项目在企业中落地的全链路能力，包括AI产品设计方法论、安全合规、监控运维及成本优化等关键环节。

**学习目标：**

- 掌握AI产品设计方法论，从用户痛点出发进行需求分析和原型设计。
- 能够进行复杂大模型项目的架构设计，合理融合RAG、Agent、Function Calling等技术。
- 理解大模型项目的安全合规、监控运维及成本优化策略。
- 具备将多个大模型技术栈融合并部署到生产环境的能力。
- 能够独立规划大模型项目的后续迭代和运营。

**详细内容：**

1. **企业级大模型项目规划与管理**
   - **AI产品设计方法论**：
     - **需求分析阶段**：如何发现用户痛点？如何从业务流程中识别AI可介入环节？用户画像构建与场景建模。
     - **原型设计阶段**：使用**Figma / Axure**制作产品原型图；构建**最小可行产品 (MVP)** 思路；设计AI对话流程图：**状态机、DSL (Domain Specific Language)**。
     - **用户反馈迭代**：**A/B测试设计**；用户满意度调查；**日志回放 + 人工审核机制**。
   - 项目生命周期：**POC (概念验证)、MVP、持续迭代**。
   - 团队协作与敏捷开发：**Scrum/Kanban**。
2. **大模型安全与合规**
   - 数据隐私与脱敏：**数据分类分级、加密存储、差分隐私**。
   - 内容审核与模型偏见：**内容过滤、Guardrails**。
   - 负责任AI原则与实践。
3. **监控、日志与可观测性**
   - 大模型服务监控指标：**QPS, 延迟, 错误率, GPU利用率, 显存占用**。
   - 日志系统：**ELK Stack (Elasticsearch, Logstash, Kibana) / Grafana Loki**。
   - 追踪系统：**OpenTelemetry, LangChain Callback Manager**。
   - A/B测试与模型迭代。
4. **成本优化与资源管理**
   - 云计算资源优化：**GPU实例选择、弹性伸缩、竞价实例**。
   - 模型量化与剪枝：**INT8/INT4量化、蒸馏** (减少模型大小与计算量)。
   - 多卡部署与负载均衡：**DeepSpeed、PyTorch FSDP**。

**实践练习：**

1. **实践一：企业级智能法律咨询助手——需求分析与架构设计**
   - 任务：针对“企业级智能法律咨询助手”项目，进行详细的需求分析，包括：
     - 识别**企业法务/合规人员**在日常工作中的痛点。
     - 分析AI在大模型背景下可以介入的法律业务环节。
     - 构建主要用户画像，并描述核心使用场景。
   - 任务：完成系统的**高层架构设计**，包括：
     - 前端、后端、大模型服务、RAG模块、Agent模块、外部工具/API、n8n集成等各个组件的划分与职责。
     - 数据流向图和关键模块之间的交互方式。
     - 技术栈选择的理由（例如：为什么选择FastAPI作为后端，VLLM作为推理引擎）。
2. **实践二：企业级智能法律咨询助手——核心功能实现与部署模拟**
   - 任务：选择“法律条文检索”或“合同条款分析”作为核心功能点，进行**代码实现**（可基于前面模块的RAG/Agent代码进行扩展）。
   - 任务：模拟项目的**部署流程**，例如：
     - 编写Docker Compose文件，启动一个包含VLLM、PGVector数据库和FastAPI后端的最小服务。
     - 模拟进行简单的API测试，验证服务是否正常运行。
   - 任务：设计一份简要的**后续迭代规划**，包括：
     - 未来可能的功能扩展（如：语音交互、法律文书自动生成）。
     - 模型持续优化策略（如：如何收集用户反馈进行再微调）。
     - 部署后的运维考虑（如：监控报警、日志分析）。
